## 概述

由于Seq2Seq在机器翻译等领域的成功应用，把这种方法用到类似的语法纠错问题上也是非常自然的想法。
机器翻译的输入是源语言(比如英语)，输出是另外一个目标语言(比如法语)。
而语法纠错的输入是有语法错误的句子，输出是与之对应的语法正确的句子，区别似乎只在于机器翻译的输入输出是不同的语言而语法纠错的输入输出是相同的语言。
随着Transformer在机器翻译领域的成功，主流的语法纠错也都使用了Transformer来作为Seq2Seq模型的Encoder和Decoder。
当然随着BERT等Pretraining模型的出现，机器翻译和语法纠错都使用了这些Pretraining的Transformer模型来作为初始化参数，并且使用领域的数据进行Fine-Tuning。
由于领域数据相对Pretraining的无监督数据量太少，最近合成的(synthetic)数据用于Fine-tuning变得流行起来。
查看一下nlpprogress的GEC任务，排行榜里的方法大多都是使用了BERT等Pretraining的Seq2Seq模型。

## seq2seq 缺点

但是Seq2Seq模型有如下缺点：

解码速度慢
因为解码不能并行计算
需要大量训练数据
因为输出的长度不定，相对本文的序列标签模型需要更多的数据
不可解释
输入了错误的句子，输出只是正确的句子，不能直接知道到底是什么类型的语法错误，通常还需要使用其它工具来分析错误，比如errant。

## gector

gector思路是使用序列标签模型替代生成模型。注意：我这里使用的是序列标签而不是更常见的序列标注来翻译Sequence Tagging，原因在于它和用来解决NER等问题的序列标注不同。序列标注的标签通常是有关联的，比如以”BIO”三标签为例，I只能出现在B或者I后面，它们的组合是有意义的。
而本文的给每一个Token打的标签和前后的标签没有关联，当然给当前Token打标签需要参考上下文，但这只是在输入层面，而在标签层面是无关的。本文的训练分为三个阶段：在合成数据上的Pretraining；
在错误-正确的句对上的fine-tuning；在同时包含错误-正确和正确-正确句对数据上的fine-tuning。


怎么把纠错问题用序列标注来解决呢？我们的数据是有语法错误和语法正确的两个句子。和机器翻译不同，语法纠错的两个句子通常非常相似，只是在某些局部会有不同的地方。因此类似于比较两个句子的diff，我们可以找到一系列编辑操作，从而把语法错误的句子变成语法正确的句子，这和编辑距离的编辑很类似。编辑操作怎么变成序列打标签呢？我们可以把编辑映射某个Token上，认为是对这个Token的操作。但是这里还有一个问题，有时候需要对同一个Token进行多个编辑操作，因为序列打标签的输出只能是一个，那怎么办呢？本文采取了一种迭代的方法，也就是通过多次(其实最多也就两三次)序列打标签。说起来有点抽象，我们来看一个例子。

![image](1.png)

比如上图的例子，红色的句子是语法错误的句子：”A ten years old boy go school”。

我们先经过一次序列打标签，找到了需要对ten和go进行操作，也就是把ten和years合并成ten-years，把go变成goes。注意：这里的用连字符”-“把两个词合并的操作定义在前面的Token上。

接着再进行一次序列打标签，发现需要对ten-years和goes进行操作，把ten-years变成ten-year然后与old合并，在goes后面增加to。

最后一次序列打标签在school后面增加句号”.”。


## 工程状态: 测试中...